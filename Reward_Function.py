import numpy as np

class RewardCalculator:
    """
    Calculates the reward for the active suspension RL agent.

    This class implements the multi-objective reward function that balances
    ride comfort, road handling, energy regeneration, and control effort.
    The weights for each objective can be tuned during initialization to
    prioritize different performance aspects (e.g., comfort vs. energy saving).
    """
    def __init__(self, w_comfort=1.0, w_holding=1.0, w_regen=0.001, w_effort=1e-6):
        """
        Initializes the reward calculator with weights for each objective.

        The default weights are chosen as a starting point and will likely
        need tuning. The weights are scaled to bring the penalties and
        rewards to a comparable order of magnitude.

        Args:
            w_comfort (float): Weight for the ride comfort penalty (body acceleration).
            w_holding (float): Weight for the road holding penalty (tire deflection).
            w_regen (float): Weight for the energy regeneration reward.
            w_effort (float): Weight for the control effort penalty (actuator force).
        """
        self.w_comfort = w_comfort
        self.w_holding = w_holding
        self.w_regen = w_regen
        self.w_effort = w_effort

    def calculate_reward(self, p_regen, x_s_ddot, x_u, x_g, u):
        """
        Calculates the total reward for a given state and action.

        Args:
            p_regen (float): The power regenerated by the actuator (W).
            x_s_ddot (float): The sprung mass (body) acceleration (m/s^2).
            x_u (float): The unsprung mass (wheel) displacement (m).
            x_g (float): The road profile displacement (m).
            u (float): The control force applied by the actuator (N).

        Returns:
            float: The total calculated reward for the current time step.
        """
        # 1. Reward for Energy Regeneration (Positive Reward)
        # Directly proportional to the watts of power harvested.
        reward_energy = self.w_regen * p_regen

        # 2. Penalty for Discomfort (Negative Reward)
        # Penalizes the square of the body acceleration to promote a smooth ride.
        penalty_comfort = self.w_comfort * (x_s_ddot ** 2)

        # 3. Penalty for Poor Handling (Negative Reward)
        # Penalizes the square of tire deflection to keep the wheel on the road.
        tire_deflection = x_u - x_g
        penalty_holding = self.w_holding * (tire_deflection ** 2)
        
        # 4. Penalty for Control Effort (Negative Reward)
        # Penalizes the square of the actuator force to encourage efficiency.
        penalty_effort = self.w_effort * (u ** 2)

        # Calculate the total reward
        total_reward = reward_energy - penalty_comfort - penalty_holding - penalty_effort

        return total_reward

# --- Example Usage ---
if __name__ == '__main__':
    # Instantiate the calculator with default weights
    reward_calc = RewardCalculator()

    # --- Simulate a scenario ---
    
    # Scenario 1: Good performance (low acceleration, good handling, some regen)
    p_regen_good = 50.0   # 50W regenerated
    x_s_ddot_good = 0.5   # 0.5 m/s^2 acceleration
    x_u_good = 0.01       # wheel is 1cm from road
    x_g_good = 0.0        # road is flat here
    u_good = -300.0       # moderate force
    
    reward_good = reward_calc.calculate_reward(p_regen_good, x_s_ddot_good, x_u_good, x_g_good, u_good)
    
    print("--- Scenario 1: Good Performance ---")
    print(f"Total Reward: {reward_good:.4f}\n")

    # Scenario 2: Bad performance (high acceleration, poor handling, no regen)
    p_regen_bad = 0.0
    x_s_ddot_bad = 8.0    # Very jerky ride
    x_u_bad = 0.05        # wheel has hopped 5cm off the road
    x_g_bad = 0.0
    u_bad = 1500.0        # high force
    
    reward_bad = reward_calc.calculate_reward(p_regen_bad, x_s_ddot_bad, x_u_bad, x_g_bad, u_bad)

    print("--- Scenario 2: Bad Performance ---")
    print(f"Total Reward: {reward_bad:.4f}\n")
    
    # Demonstrate how changing weights affects priorities
    print("--- Demonstrating Weight Priority ---")
    # "Eco Mode" - prioritize energy regeneration heavily
    eco_reward_calc = RewardCalculator(w_regen=0.1, w_comfort=0.1, w_holding=0.1)
    eco_reward = eco_reward_calc.calculate_reward(p_regen_good, x_s_ddot_good, x_u_good, x_g_good, u_good)
    print(f"Reward in 'Eco Mode': {eco_reward:.4f}")
    
    # "Comfort Mode" - prioritize comfort heavily
    comfort_reward_calc = RewardCalculator(w_regen=0.001, w_comfort=10.0, w_holding=1.0)
    comfort_reward = comfort_reward_calc.calculate_reward(p_regen_good, x_s_ddot_good, x_u_good, x_g_good, u_good)
    print(f"Reward in 'Comfort Mode': {comfort_reward:.4f}")

